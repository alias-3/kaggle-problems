{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "titanic-kernel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPtm1huHbpv8",
        "colab_type": "code",
        "outputId": "9ca44646-f2c3-43e6-87f7-7781233532a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "eVI-IiZrabP7",
        "colab_type": "code",
        "outputId": "6a66a113-d164-4918-e89d-ccaf06e5ec84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import accuracy_score,mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,BatchNormalization\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# import os\n",
        "# print(os.listdir(\"../input\"))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "f015690e940ef1f600d47cfa8de981249e310646",
        "id": "3IgeWFZCabQG",
        "colab_type": "code",
        "outputId": "5addf348-92c6-4463-98da-26f69d0378a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# train_data = pd.read_csv(\"../input/train.csv\")\n",
        "train_data = pd.read_csv(\"drive/My Drive/Colab Notebooks/train.csv\")\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
              "0            1         0       3  ...   7.2500   NaN         S\n",
              "1            2         1       1  ...  71.2833   C85         C\n",
              "2            3         1       3  ...   7.9250   NaN         S\n",
              "3            4         1       1  ...  53.1000  C123         S\n",
              "4            5         0       3  ...   8.0500   NaN         S\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpwNQGd5d6Sg",
        "colab_type": "code",
        "outputId": "a14f2184-2afa-4221-82ca-942ce24af64b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# my_test_data = pd.read_csv(\"../input/test.csv\")\n",
        "test_data = pd.read_csv(\"drive/My Drive/Colab Notebooks/test.csv\")\n",
        "orig_test = test_data.copy()\n",
        "test_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>3</td>\n",
              "      <td>Kelly, Mr. James</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330911</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>893</td>\n",
              "      <td>3</td>\n",
              "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>363272</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>894</td>\n",
              "      <td>2</td>\n",
              "      <td>Myles, Mr. Thomas Francis</td>\n",
              "      <td>male</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>240276</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>895</td>\n",
              "      <td>3</td>\n",
              "      <td>Wirz, Mr. Albert</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>315154</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>896</td>\n",
              "      <td>3</td>\n",
              "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
              "      <td>female</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3101298</td>\n",
              "      <td>12.2875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Pclass  ... Cabin Embarked\n",
              "0          892       3  ...   NaN        Q\n",
              "1          893       3  ...   NaN        S\n",
              "2          894       2  ...   NaN        Q\n",
              "3          895       3  ...   NaN        S\n",
              "4          896       3  ...   NaN        S\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwpcr1uxdR41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = train_data['Survived']\n",
        "train_data.drop(['PassengerId','Ticket','Name'],axis=1,inplace=True)\n",
        "test_data.drop(['PassengerId','Ticket','Name'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3LI4WGKfo0D",
        "colab_type": "code",
        "outputId": "5df1143a-ce8f-43a3-9ca3-ec3f53ccf47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "train_data1 = train_data.copy()\n",
        "corr = train_data.corr()\n",
        "\n",
        "sns.set(font_scale = 1)\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "sns.heatmap(corr)\n",
        "plt.title('Correlation Plot')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KwvnStp0BtW",
        "colab_type": "code",
        "outputId": "2230daa2-46cd-4672-ffb2-1de6e32ee99a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "abs(corr).Survived.drop('Survived').sort_values().plot.barh()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb00a8cbfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHOR38PE5Typ",
        "colab_type": "code",
        "outputId": "30afe75a-6309-48f7-d6ac-43f8fce95312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "dummy_fields = ['Pclass', 'Embarked']\n",
        "for each in dummy_fields:\n",
        "    dummies = pd.get_dummies(train_data[each], prefix=each, drop_first=False)\n",
        "    train_data = pd.concat([train_data, dummies], axis=1)\n",
        "\n",
        "for each in dummy_fields:\n",
        "    dummies = pd.get_dummies(test_data[each], prefix=each, drop_first=False)    \n",
        "    test_data = pd.concat([test_data, dummies], axis=1)\n",
        "\n",
        "fields_to_drop = ['Pclass', 'Embarked','Survived']\n",
        "train_data.drop(fields_to_drop, axis=1,inplace=True)\n",
        "test_data.drop(fields_to_drop[:2], axis=1,inplace=True)\n",
        "\n",
        "print('train_info\\n')\n",
        "train_data.info()\n",
        "print('\\n\\ntest_info')\n",
        "test_data.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_info\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Sex         891 non-null    object \n",
            " 1   Age         714 non-null    float64\n",
            " 2   SibSp       891 non-null    int64  \n",
            " 3   Parch       891 non-null    int64  \n",
            " 4   Fare        891 non-null    float64\n",
            " 5   Cabin       204 non-null    object \n",
            " 6   Pclass_1    891 non-null    uint8  \n",
            " 7   Pclass_2    891 non-null    uint8  \n",
            " 8   Pclass_3    891 non-null    uint8  \n",
            " 9   Embarked_C  891 non-null    uint8  \n",
            " 10  Embarked_Q  891 non-null    uint8  \n",
            " 11  Embarked_S  891 non-null    uint8  \n",
            "dtypes: float64(2), int64(2), object(2), uint8(6)\n",
            "memory usage: 47.1+ KB\n",
            "\n",
            "\n",
            "test_info\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 418 entries, 0 to 417\n",
            "Data columns (total 12 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Sex         418 non-null    object \n",
            " 1   Age         332 non-null    float64\n",
            " 2   SibSp       418 non-null    int64  \n",
            " 3   Parch       418 non-null    int64  \n",
            " 4   Fare        417 non-null    float64\n",
            " 5   Cabin       91 non-null     object \n",
            " 6   Pclass_1    418 non-null    uint8  \n",
            " 7   Pclass_2    418 non-null    uint8  \n",
            " 8   Pclass_3    418 non-null    uint8  \n",
            " 9   Embarked_C  418 non-null    uint8  \n",
            " 10  Embarked_Q  418 non-null    uint8  \n",
            " 11  Embarked_S  418 non-null    uint8  \n",
            "dtypes: float64(2), int64(2), object(2), uint8(6)\n",
            "memory usage: 22.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgN11o7G8-M6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.drop(['Cabin'],axis=1,inplace=True)\n",
        "train_data.replace(to_replace=\"female\", value=1, inplace=True)\n",
        "train_data.replace(to_replace=\"male\", value=0, inplace=True)\n",
        "train_data['Age'].fillna(train_data.Age.mean(),inplace=True)\n",
        "\n",
        "test_data.drop(['Cabin'],axis=1,inplace=True)\n",
        "test_data.replace(to_replace=\"female\", value=1, inplace=True)\n",
        "test_data.replace(to_replace=\"male\", value=0, inplace=True)\n",
        "test_data['Age'].fillna(test_data.Age.mean(),inplace=True)\n",
        "test_data['Fare'].fillna(test_data.Fare.mean(),inplace=True)\n",
        "\n",
        "train_data_nonorm = train_data.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNTfOVKyVFmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "train_data[:] = MinMaxScaler().fit_transform(train_data[:])\n",
        "test_data[:] = MinMaxScaler().fit_transform(test_data[:]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQm8kg5rCD7V",
        "colab_type": "code",
        "outputId": "7eb800cd-16bb-4806-9c86-25065e922110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "print('\\ntrain_head')\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train_head\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Pclass_1</th>\n",
              "      <th>Pclass_2</th>\n",
              "      <th>Pclass_3</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.271174</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.014151</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.472229</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.139136</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.321438</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015469</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.434531</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.103644</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.434531</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015713</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sex       Age  SibSp  Parch  ...  Pclass_3  Embarked_C  Embarked_Q  Embarked_S\n",
              "0  0.0  0.271174  0.125    0.0  ...       1.0         0.0         0.0         1.0\n",
              "1  1.0  0.472229  0.125    0.0  ...       0.0         1.0         0.0         0.0\n",
              "2  1.0  0.321438  0.000    0.0  ...       1.0         0.0         0.0         1.0\n",
              "3  1.0  0.434531  0.125    0.0  ...       0.0         0.0         0.0         1.0\n",
              "4  0.0  0.434531  0.000    0.0  ...       1.0         0.0         0.0         1.0\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyV2COprCGac",
        "colab_type": "code",
        "outputId": "1ae46f1d-8bec-48af-9b8d-e9a88592d244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "print('\\ntest_head')\n",
        "test_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test_head\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Pclass_1</th>\n",
              "      <th>Pclass_2</th>\n",
              "      <th>Pclass_3</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.452723</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015282</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.617566</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013663</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.815377</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018909</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353818</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016908</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.287881</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.023984</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sex       Age  SibSp     Parch  ...  Pclass_3  Embarked_C  Embarked_Q  Embarked_S\n",
              "0  0.0  0.452723  0.000  0.000000  ...       1.0         0.0         1.0         0.0\n",
              "1  1.0  0.617566  0.125  0.000000  ...       1.0         0.0         0.0         1.0\n",
              "2  0.0  0.815377  0.000  0.000000  ...       0.0         0.0         1.0         0.0\n",
              "3  0.0  0.353818  0.000  0.000000  ...       1.0         0.0         0.0         1.0\n",
              "4  1.0  0.287881  0.125  0.111111  ...       1.0         0.0         0.0         1.0\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c880294044332dcc13d0d182fda6ed4ff5e995b9",
        "id": "oo5U_rXTabQM",
        "colab_type": "code",
        "outputId": "80593b67-7430-467e-e31c-95a4cffa6bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "train_X,test_X,train_y,test_y = train_test_split(train_data,train_labels,test_size = 0.2,random_state=1)\n",
        "train_X.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 712 entries, 301 to 37\n",
            "Data columns (total 11 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Sex         712 non-null    float64\n",
            " 1   Age         712 non-null    float64\n",
            " 2   SibSp       712 non-null    float64\n",
            " 3   Parch       712 non-null    float64\n",
            " 4   Fare        712 non-null    float64\n",
            " 5   Pclass_1    712 non-null    float64\n",
            " 6   Pclass_2    712 non-null    float64\n",
            " 7   Pclass_3    712 non-null    float64\n",
            " 8   Embarked_C  712 non-null    float64\n",
            " 9   Embarked_Q  712 non-null    float64\n",
            " 10  Embarked_S  712 non-null    float64\n",
            "dtypes: float64(11)\n",
            "memory usage: 66.8 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPD25iwETzmH",
        "colab_type": "code",
        "outputId": "5ddb50d6-55ac-4f2b-cd52-87e92fdd8009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "train_X.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Pclass_1</th>\n",
              "      <th>Pclass_2</th>\n",
              "      <th>Pclass_3</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>712.000000</td>\n",
              "      <td>712.000000</td>\n",
              "      <td>712.000000</td>\n",
              "      <td>712.000000</td>\n",
              "      <td>712.000000</td>\n",
              "      <td>712.000000</td>\n",
              "      <td>712.000000</td>\n",
              "      <td>712.000000</td>\n",
              "      <td>712.000000</td>\n",
              "      <td>712.000000</td>\n",
              "      <td>712.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.349719</td>\n",
              "      <td>0.372603</td>\n",
              "      <td>0.061798</td>\n",
              "      <td>0.061798</td>\n",
              "      <td>0.062365</td>\n",
              "      <td>0.244382</td>\n",
              "      <td>0.210674</td>\n",
              "      <td>0.544944</td>\n",
              "      <td>0.186798</td>\n",
              "      <td>0.089888</td>\n",
              "      <td>0.720506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.477216</td>\n",
              "      <td>0.166728</td>\n",
              "      <td>0.126310</td>\n",
              "      <td>0.129328</td>\n",
              "      <td>0.090399</td>\n",
              "      <td>0.430022</td>\n",
              "      <td>0.408074</td>\n",
              "      <td>0.498326</td>\n",
              "      <td>0.390023</td>\n",
              "      <td>0.286222</td>\n",
              "      <td>0.449066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.271174</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015412</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.367921</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.447097</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.059914</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Sex         Age       SibSp  ...  Embarked_C  Embarked_Q  Embarked_S\n",
              "count  712.000000  712.000000  712.000000  ...  712.000000  712.000000  712.000000\n",
              "mean     0.349719    0.372603    0.061798  ...    0.186798    0.089888    0.720506\n",
              "std      0.477216    0.166728    0.126310  ...    0.390023    0.286222    0.449066\n",
              "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
              "25%      0.000000    0.271174    0.000000  ...    0.000000    0.000000    0.000000\n",
              "50%      0.000000    0.367921    0.000000  ...    0.000000    0.000000    1.000000\n",
              "75%      1.000000    0.447097    0.125000  ...    0.000000    0.000000    1.000000\n",
              "max      1.000000    1.000000    1.000000  ...    1.000000    1.000000    1.000000\n",
              "\n",
              "[8 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4ja3vq9XMb2",
        "colab_type": "code",
        "outputId": "19508ad6-bad3-4c68-ea54-75ac7eaddadd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Neural network\n",
        "act = 'relu'\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(16, input_dim=11, activation=act))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(16, activation=act))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(16, activation=act))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(8, activation=act))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "from keras.optimizers import SGD,Adam\n",
        "opt = Adam(lr=0.01)\n",
        "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "history = model.fit(train_X,train_y,epochs=300,batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 16)                192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 1,105\n",
            "Trainable params: 993\n",
            "Non-trainable params: 112\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.1976 - accuracy: 0.7331\n",
            "Epoch 2/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1462 - accuracy: 0.8104\n",
            "Epoch 3/300\n",
            "712/712 [==============================] - 0s 131us/step - loss: 0.1382 - accuracy: 0.8146\n",
            "Epoch 4/300\n",
            "712/712 [==============================] - 0s 115us/step - loss: 0.1308 - accuracy: 0.8329\n",
            "Epoch 5/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1286 - accuracy: 0.8315\n",
            "Epoch 6/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1299 - accuracy: 0.8202\n",
            "Epoch 7/300\n",
            "712/712 [==============================] - 0s 122us/step - loss: 0.1252 - accuracy: 0.8399\n",
            "Epoch 8/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1293 - accuracy: 0.8287\n",
            "Epoch 9/300\n",
            "712/712 [==============================] - 0s 110us/step - loss: 0.1234 - accuracy: 0.8399\n",
            "Epoch 10/300\n",
            "712/712 [==============================] - 0s 118us/step - loss: 0.1291 - accuracy: 0.8244\n",
            "Epoch 11/300\n",
            "712/712 [==============================] - 0s 124us/step - loss: 0.1246 - accuracy: 0.8272\n",
            "Epoch 12/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1298 - accuracy: 0.8315\n",
            "Epoch 13/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1302 - accuracy: 0.8385\n",
            "Epoch 14/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1247 - accuracy: 0.8427\n",
            "Epoch 15/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1261 - accuracy: 0.8371\n",
            "Epoch 16/300\n",
            "712/712 [==============================] - 0s 98us/step - loss: 0.1263 - accuracy: 0.8371\n",
            "Epoch 17/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1327 - accuracy: 0.8287\n",
            "Epoch 18/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1264 - accuracy: 0.8287\n",
            "Epoch 19/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1233 - accuracy: 0.8511\n",
            "Epoch 20/300\n",
            "712/712 [==============================] - 0s 102us/step - loss: 0.1204 - accuracy: 0.8455\n",
            "Epoch 21/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1276 - accuracy: 0.8343\n",
            "Epoch 22/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1315 - accuracy: 0.8287\n",
            "Epoch 23/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1273 - accuracy: 0.8301\n",
            "Epoch 24/300\n",
            "712/712 [==============================] - 0s 109us/step - loss: 0.1334 - accuracy: 0.8272\n",
            "Epoch 25/300\n",
            "712/712 [==============================] - 0s 92us/step - loss: 0.1265 - accuracy: 0.8272\n",
            "Epoch 26/300\n",
            "712/712 [==============================] - 0s 88us/step - loss: 0.1251 - accuracy: 0.8343\n",
            "Epoch 27/300\n",
            "712/712 [==============================] - 0s 112us/step - loss: 0.1237 - accuracy: 0.8357\n",
            "Epoch 28/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1275 - accuracy: 0.8301\n",
            "Epoch 29/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1247 - accuracy: 0.8329\n",
            "Epoch 30/300\n",
            "712/712 [==============================] - 0s 102us/step - loss: 0.1247 - accuracy: 0.8427\n",
            "Epoch 31/300\n",
            "712/712 [==============================] - 0s 124us/step - loss: 0.1225 - accuracy: 0.8399\n",
            "Epoch 32/300\n",
            "712/712 [==============================] - 0s 122us/step - loss: 0.1240 - accuracy: 0.8441\n",
            "Epoch 33/300\n",
            "712/712 [==============================] - 0s 122us/step - loss: 0.1282 - accuracy: 0.8315\n",
            "Epoch 34/300\n",
            "712/712 [==============================] - 0s 119us/step - loss: 0.1237 - accuracy: 0.8399\n",
            "Epoch 35/300\n",
            "712/712 [==============================] - 0s 135us/step - loss: 0.1186 - accuracy: 0.8399\n",
            "Epoch 36/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1225 - accuracy: 0.8371\n",
            "Epoch 37/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1240 - accuracy: 0.8315\n",
            "Epoch 38/300\n",
            "712/712 [==============================] - 0s 110us/step - loss: 0.1295 - accuracy: 0.8343\n",
            "Epoch 39/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1311 - accuracy: 0.8315\n",
            "Epoch 40/300\n",
            "712/712 [==============================] - 0s 130us/step - loss: 0.1260 - accuracy: 0.8343\n",
            "Epoch 41/300\n",
            "712/712 [==============================] - 0s 114us/step - loss: 0.1232 - accuracy: 0.8329\n",
            "Epoch 42/300\n",
            "712/712 [==============================] - 0s 110us/step - loss: 0.1246 - accuracy: 0.8455\n",
            "Epoch 43/300\n",
            "712/712 [==============================] - 0s 123us/step - loss: 0.1226 - accuracy: 0.8287\n",
            "Epoch 44/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1259 - accuracy: 0.8357\n",
            "Epoch 45/300\n",
            "712/712 [==============================] - 0s 100us/step - loss: 0.1268 - accuracy: 0.8272\n",
            "Epoch 46/300\n",
            "712/712 [==============================] - 0s 95us/step - loss: 0.1198 - accuracy: 0.8441\n",
            "Epoch 47/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1240 - accuracy: 0.8385\n",
            "Epoch 48/300\n",
            "712/712 [==============================] - 0s 112us/step - loss: 0.1234 - accuracy: 0.8371\n",
            "Epoch 49/300\n",
            "712/712 [==============================] - 0s 110us/step - loss: 0.1266 - accuracy: 0.8413\n",
            "Epoch 50/300\n",
            "712/712 [==============================] - 0s 115us/step - loss: 0.1249 - accuracy: 0.8427\n",
            "Epoch 51/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1267 - accuracy: 0.8272\n",
            "Epoch 52/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1272 - accuracy: 0.8343\n",
            "Epoch 53/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1179 - accuracy: 0.8483\n",
            "Epoch 54/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1258 - accuracy: 0.8357\n",
            "Epoch 55/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1216 - accuracy: 0.8441\n",
            "Epoch 56/300\n",
            "712/712 [==============================] - 0s 104us/step - loss: 0.1239 - accuracy: 0.8441\n",
            "Epoch 57/300\n",
            "712/712 [==============================] - 0s 91us/step - loss: 0.1200 - accuracy: 0.8455\n",
            "Epoch 58/300\n",
            "712/712 [==============================] - 0s 127us/step - loss: 0.1151 - accuracy: 0.8553\n",
            "Epoch 59/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1222 - accuracy: 0.8455\n",
            "Epoch 60/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1207 - accuracy: 0.8441\n",
            "Epoch 61/300\n",
            "712/712 [==============================] - 0s 115us/step - loss: 0.1171 - accuracy: 0.8497\n",
            "Epoch 62/300\n",
            "712/712 [==============================] - 0s 104us/step - loss: 0.1233 - accuracy: 0.8413\n",
            "Epoch 63/300\n",
            "712/712 [==============================] - 0s 102us/step - loss: 0.1269 - accuracy: 0.8399\n",
            "Epoch 64/300\n",
            "712/712 [==============================] - 0s 100us/step - loss: 0.1240 - accuracy: 0.8385\n",
            "Epoch 65/300\n",
            "712/712 [==============================] - 0s 131us/step - loss: 0.1199 - accuracy: 0.8455\n",
            "Epoch 66/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1223 - accuracy: 0.8371\n",
            "Epoch 67/300\n",
            "712/712 [==============================] - 0s 101us/step - loss: 0.1182 - accuracy: 0.8441\n",
            "Epoch 68/300\n",
            "712/712 [==============================] - 0s 104us/step - loss: 0.1186 - accuracy: 0.8455\n",
            "Epoch 69/300\n",
            "712/712 [==============================] - 0s 96us/step - loss: 0.1247 - accuracy: 0.8399\n",
            "Epoch 70/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1232 - accuracy: 0.8399\n",
            "Epoch 71/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1193 - accuracy: 0.8483\n",
            "Epoch 72/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1178 - accuracy: 0.8497\n",
            "Epoch 73/300\n",
            "712/712 [==============================] - 0s 100us/step - loss: 0.1191 - accuracy: 0.8497\n",
            "Epoch 74/300\n",
            "712/712 [==============================] - 0s 99us/step - loss: 0.1154 - accuracy: 0.8483\n",
            "Epoch 75/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1170 - accuracy: 0.8567\n",
            "Epoch 76/300\n",
            "712/712 [==============================] - 0s 89us/step - loss: 0.1245 - accuracy: 0.8399\n",
            "Epoch 77/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1199 - accuracy: 0.8497\n",
            "Epoch 78/300\n",
            "712/712 [==============================] - 0s 100us/step - loss: 0.1381 - accuracy: 0.8160\n",
            "Epoch 79/300\n",
            "712/712 [==============================] - 0s 126us/step - loss: 0.1268 - accuracy: 0.8343\n",
            "Epoch 80/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1296 - accuracy: 0.8385\n",
            "Epoch 81/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1240 - accuracy: 0.8385\n",
            "Epoch 82/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1198 - accuracy: 0.8441\n",
            "Epoch 83/300\n",
            "712/712 [==============================] - 0s 118us/step - loss: 0.1236 - accuracy: 0.8441\n",
            "Epoch 84/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1234 - accuracy: 0.8399\n",
            "Epoch 85/300\n",
            "712/712 [==============================] - 0s 109us/step - loss: 0.1193 - accuracy: 0.8385\n",
            "Epoch 86/300\n",
            "712/712 [==============================] - 0s 98us/step - loss: 0.1250 - accuracy: 0.8385\n",
            "Epoch 87/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1228 - accuracy: 0.8413\n",
            "Epoch 88/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1263 - accuracy: 0.8329\n",
            "Epoch 89/300\n",
            "712/712 [==============================] - 0s 104us/step - loss: 0.1220 - accuracy: 0.8399\n",
            "Epoch 90/300\n",
            "712/712 [==============================] - 0s 98us/step - loss: 0.1192 - accuracy: 0.8483\n",
            "Epoch 91/300\n",
            "712/712 [==============================] - 0s 98us/step - loss: 0.1198 - accuracy: 0.8427\n",
            "Epoch 92/300\n",
            "712/712 [==============================] - 0s 122us/step - loss: 0.1217 - accuracy: 0.8483\n",
            "Epoch 93/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1239 - accuracy: 0.8371\n",
            "Epoch 94/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1178 - accuracy: 0.8553\n",
            "Epoch 95/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1180 - accuracy: 0.8469\n",
            "Epoch 96/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1207 - accuracy: 0.8427\n",
            "Epoch 97/300\n",
            "712/712 [==============================] - 0s 112us/step - loss: 0.1231 - accuracy: 0.8357\n",
            "Epoch 98/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1197 - accuracy: 0.8469\n",
            "Epoch 99/300\n",
            "712/712 [==============================] - 0s 104us/step - loss: 0.1207 - accuracy: 0.8455\n",
            "Epoch 100/300\n",
            "712/712 [==============================] - 0s 109us/step - loss: 0.1207 - accuracy: 0.8455\n",
            "Epoch 101/300\n",
            "712/712 [==============================] - 0s 130us/step - loss: 0.1249 - accuracy: 0.8441\n",
            "Epoch 102/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1186 - accuracy: 0.8455\n",
            "Epoch 103/300\n",
            "712/712 [==============================] - 0s 123us/step - loss: 0.1266 - accuracy: 0.8329\n",
            "Epoch 104/300\n",
            "712/712 [==============================] - 0s 125us/step - loss: 0.1191 - accuracy: 0.8469\n",
            "Epoch 105/300\n",
            "712/712 [==============================] - 0s 133us/step - loss: 0.1174 - accuracy: 0.8539\n",
            "Epoch 106/300\n",
            "712/712 [==============================] - 0s 115us/step - loss: 0.1183 - accuracy: 0.8399\n",
            "Epoch 107/300\n",
            "712/712 [==============================] - 0s 115us/step - loss: 0.1201 - accuracy: 0.8357\n",
            "Epoch 108/300\n",
            "712/712 [==============================] - 0s 100us/step - loss: 0.1178 - accuracy: 0.8441\n",
            "Epoch 109/300\n",
            "712/712 [==============================] - 0s 78us/step - loss: 0.1183 - accuracy: 0.8427\n",
            "Epoch 110/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1153 - accuracy: 0.8497\n",
            "Epoch 111/300\n",
            "712/712 [==============================] - 0s 99us/step - loss: 0.1128 - accuracy: 0.8539\n",
            "Epoch 112/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1276 - accuracy: 0.8258\n",
            "Epoch 113/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1139 - accuracy: 0.8525\n",
            "Epoch 114/300\n",
            "712/712 [==============================] - 0s 97us/step - loss: 0.1184 - accuracy: 0.8497\n",
            "Epoch 115/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1197 - accuracy: 0.8525\n",
            "Epoch 116/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1179 - accuracy: 0.8511\n",
            "Epoch 117/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1192 - accuracy: 0.8497\n",
            "Epoch 118/300\n",
            "712/712 [==============================] - 0s 95us/step - loss: 0.1179 - accuracy: 0.8497\n",
            "Epoch 119/300\n",
            "712/712 [==============================] - 0s 104us/step - loss: 0.1181 - accuracy: 0.8455\n",
            "Epoch 120/300\n",
            "712/712 [==============================] - 0s 114us/step - loss: 0.1219 - accuracy: 0.8399\n",
            "Epoch 121/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1184 - accuracy: 0.8525\n",
            "Epoch 122/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1147 - accuracy: 0.8567\n",
            "Epoch 123/300\n",
            "712/712 [==============================] - 0s 102us/step - loss: 0.1161 - accuracy: 0.8539\n",
            "Epoch 124/300\n",
            "712/712 [==============================] - 0s 102us/step - loss: 0.1207 - accuracy: 0.8385\n",
            "Epoch 125/300\n",
            "712/712 [==============================] - 0s 96us/step - loss: 0.1150 - accuracy: 0.8581\n",
            "Epoch 126/300\n",
            "712/712 [==============================] - 0s 93us/step - loss: 0.1176 - accuracy: 0.8427\n",
            "Epoch 127/300\n",
            "712/712 [==============================] - 0s 94us/step - loss: 0.1177 - accuracy: 0.8427\n",
            "Epoch 128/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1176 - accuracy: 0.8427\n",
            "Epoch 129/300\n",
            "712/712 [==============================] - 0s 93us/step - loss: 0.1132 - accuracy: 0.8497\n",
            "Epoch 130/300\n",
            "712/712 [==============================] - 0s 131us/step - loss: 0.1133 - accuracy: 0.8483\n",
            "Epoch 131/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1200 - accuracy: 0.8483\n",
            "Epoch 132/300\n",
            "712/712 [==============================] - 0s 112us/step - loss: 0.1210 - accuracy: 0.8441\n",
            "Epoch 133/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1156 - accuracy: 0.8539\n",
            "Epoch 134/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1183 - accuracy: 0.8483\n",
            "Epoch 135/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1246 - accuracy: 0.8385\n",
            "Epoch 136/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1193 - accuracy: 0.8497\n",
            "Epoch 137/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1179 - accuracy: 0.8483\n",
            "Epoch 138/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1209 - accuracy: 0.8357\n",
            "Epoch 139/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1169 - accuracy: 0.8567\n",
            "Epoch 140/300\n",
            "712/712 [==============================] - 0s 115us/step - loss: 0.1206 - accuracy: 0.8399\n",
            "Epoch 141/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1135 - accuracy: 0.8525\n",
            "Epoch 142/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1237 - accuracy: 0.8427\n",
            "Epoch 143/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1173 - accuracy: 0.8525\n",
            "Epoch 144/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1124 - accuracy: 0.8581\n",
            "Epoch 145/300\n",
            "712/712 [==============================] - 0s 115us/step - loss: 0.1129 - accuracy: 0.8567\n",
            "Epoch 146/300\n",
            "712/712 [==============================] - 0s 117us/step - loss: 0.1151 - accuracy: 0.8483\n",
            "Epoch 147/300\n",
            "712/712 [==============================] - 0s 114us/step - loss: 0.1247 - accuracy: 0.8371\n",
            "Epoch 148/300\n",
            "712/712 [==============================] - 0s 112us/step - loss: 0.1148 - accuracy: 0.8539\n",
            "Epoch 149/300\n",
            "712/712 [==============================] - 0s 130us/step - loss: 0.1199 - accuracy: 0.8427\n",
            "Epoch 150/300\n",
            "712/712 [==============================] - 0s 104us/step - loss: 0.1140 - accuracy: 0.8539\n",
            "Epoch 151/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1186 - accuracy: 0.8441\n",
            "Epoch 152/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1160 - accuracy: 0.8497\n",
            "Epoch 153/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1211 - accuracy: 0.8399\n",
            "Epoch 154/300\n",
            "712/712 [==============================] - 0s 102us/step - loss: 0.1185 - accuracy: 0.8427\n",
            "Epoch 155/300\n",
            "712/712 [==============================] - 0s 126us/step - loss: 0.1164 - accuracy: 0.8441\n",
            "Epoch 156/300\n",
            "712/712 [==============================] - 0s 95us/step - loss: 0.1189 - accuracy: 0.8427\n",
            "Epoch 157/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1153 - accuracy: 0.8483\n",
            "Epoch 158/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1154 - accuracy: 0.8511\n",
            "Epoch 159/300\n",
            "712/712 [==============================] - 0s 99us/step - loss: 0.1166 - accuracy: 0.8483\n",
            "Epoch 160/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1153 - accuracy: 0.8483\n",
            "Epoch 161/300\n",
            "712/712 [==============================] - 0s 125us/step - loss: 0.1147 - accuracy: 0.8539\n",
            "Epoch 162/300\n",
            "712/712 [==============================] - 0s 110us/step - loss: 0.1152 - accuracy: 0.8497\n",
            "Epoch 163/300\n",
            "712/712 [==============================] - 0s 99us/step - loss: 0.1185 - accuracy: 0.8441\n",
            "Epoch 164/300\n",
            "712/712 [==============================] - 0s 101us/step - loss: 0.1169 - accuracy: 0.8525\n",
            "Epoch 165/300\n",
            "712/712 [==============================] - 0s 104us/step - loss: 0.1080 - accuracy: 0.8581\n",
            "Epoch 166/300\n",
            "712/712 [==============================] - 0s 100us/step - loss: 0.1166 - accuracy: 0.8427\n",
            "Epoch 167/300\n",
            "712/712 [==============================] - 0s 118us/step - loss: 0.1158 - accuracy: 0.8483\n",
            "Epoch 168/300\n",
            "712/712 [==============================] - 0s 122us/step - loss: 0.1180 - accuracy: 0.8483\n",
            "Epoch 169/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1208 - accuracy: 0.8455\n",
            "Epoch 170/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1122 - accuracy: 0.8511\n",
            "Epoch 171/300\n",
            "712/712 [==============================] - 0s 110us/step - loss: 0.1122 - accuracy: 0.8567\n",
            "Epoch 172/300\n",
            "712/712 [==============================] - 0s 114us/step - loss: 0.1132 - accuracy: 0.8539\n",
            "Epoch 173/300\n",
            "712/712 [==============================] - 0s 99us/step - loss: 0.1180 - accuracy: 0.8483\n",
            "Epoch 174/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1155 - accuracy: 0.8511\n",
            "Epoch 175/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1165 - accuracy: 0.8483\n",
            "Epoch 176/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1113 - accuracy: 0.8553\n",
            "Epoch 177/300\n",
            "712/712 [==============================] - 0s 102us/step - loss: 0.1147 - accuracy: 0.8525\n",
            "Epoch 178/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1218 - accuracy: 0.8427\n",
            "Epoch 179/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1129 - accuracy: 0.8567\n",
            "Epoch 180/300\n",
            "712/712 [==============================] - 0s 109us/step - loss: 0.1126 - accuracy: 0.8525\n",
            "Epoch 181/300\n",
            "712/712 [==============================] - 0s 112us/step - loss: 0.1129 - accuracy: 0.8497\n",
            "Epoch 182/300\n",
            "712/712 [==============================] - 0s 119us/step - loss: 0.1178 - accuracy: 0.8511\n",
            "Epoch 183/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1162 - accuracy: 0.8539\n",
            "Epoch 184/300\n",
            "712/712 [==============================] - 0s 104us/step - loss: 0.1126 - accuracy: 0.8497\n",
            "Epoch 185/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1167 - accuracy: 0.8441\n",
            "Epoch 186/300\n",
            "712/712 [==============================] - 0s 118us/step - loss: 0.1186 - accuracy: 0.8511\n",
            "Epoch 187/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1105 - accuracy: 0.8652\n",
            "Epoch 188/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1160 - accuracy: 0.8497\n",
            "Epoch 189/300\n",
            "712/712 [==============================] - 0s 100us/step - loss: 0.1194 - accuracy: 0.8455\n",
            "Epoch 190/300\n",
            "712/712 [==============================] - 0s 119us/step - loss: 0.1151 - accuracy: 0.8469\n",
            "Epoch 191/300\n",
            "712/712 [==============================] - 0s 95us/step - loss: 0.1143 - accuracy: 0.8525\n",
            "Epoch 192/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1124 - accuracy: 0.8596\n",
            "Epoch 193/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1192 - accuracy: 0.8511\n",
            "Epoch 194/300\n",
            "712/712 [==============================] - 0s 112us/step - loss: 0.1119 - accuracy: 0.8581\n",
            "Epoch 195/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1137 - accuracy: 0.8553\n",
            "Epoch 196/300\n",
            "712/712 [==============================] - 0s 104us/step - loss: 0.1128 - accuracy: 0.8553\n",
            "Epoch 197/300\n",
            "712/712 [==============================] - 0s 125us/step - loss: 0.1170 - accuracy: 0.8427\n",
            "Epoch 198/300\n",
            "712/712 [==============================] - 0s 91us/step - loss: 0.1111 - accuracy: 0.8525\n",
            "Epoch 199/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1120 - accuracy: 0.8553\n",
            "Epoch 200/300\n",
            "712/712 [==============================] - 0s 96us/step - loss: 0.1191 - accuracy: 0.8441\n",
            "Epoch 201/300\n",
            "712/712 [==============================] - 0s 138us/step - loss: 0.1139 - accuracy: 0.8581\n",
            "Epoch 202/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1140 - accuracy: 0.8553\n",
            "Epoch 203/300\n",
            "712/712 [==============================] - 0s 109us/step - loss: 0.1143 - accuracy: 0.8525\n",
            "Epoch 204/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1144 - accuracy: 0.8596\n",
            "Epoch 205/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1133 - accuracy: 0.8525\n",
            "Epoch 206/300\n",
            "712/712 [==============================] - 0s 94us/step - loss: 0.1147 - accuracy: 0.8525\n",
            "Epoch 207/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1156 - accuracy: 0.8539\n",
            "Epoch 208/300\n",
            "712/712 [==============================] - 0s 95us/step - loss: 0.1144 - accuracy: 0.8427\n",
            "Epoch 209/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1160 - accuracy: 0.8539\n",
            "Epoch 210/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1165 - accuracy: 0.8553\n",
            "Epoch 211/300\n",
            "712/712 [==============================] - 0s 114us/step - loss: 0.1133 - accuracy: 0.8525\n",
            "Epoch 212/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1136 - accuracy: 0.8539\n",
            "Epoch 213/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1129 - accuracy: 0.8539\n",
            "Epoch 214/300\n",
            "712/712 [==============================] - 0s 100us/step - loss: 0.1125 - accuracy: 0.8525\n",
            "Epoch 215/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1107 - accuracy: 0.8581\n",
            "Epoch 216/300\n",
            "712/712 [==============================] - 0s 104us/step - loss: 0.1135 - accuracy: 0.8581\n",
            "Epoch 217/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1102 - accuracy: 0.8581\n",
            "Epoch 218/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1085 - accuracy: 0.8610\n",
            "Epoch 219/300\n",
            "712/712 [==============================] - 0s 99us/step - loss: 0.1099 - accuracy: 0.8596\n",
            "Epoch 220/300\n",
            "712/712 [==============================] - 0s 121us/step - loss: 0.1163 - accuracy: 0.8427\n",
            "Epoch 221/300\n",
            "712/712 [==============================] - 0s 124us/step - loss: 0.1166 - accuracy: 0.8469\n",
            "Epoch 222/300\n",
            "712/712 [==============================] - 0s 134us/step - loss: 0.1101 - accuracy: 0.8525\n",
            "Epoch 223/300\n",
            "712/712 [==============================] - 0s 115us/step - loss: 0.1148 - accuracy: 0.8469\n",
            "Epoch 224/300\n",
            "712/712 [==============================] - 0s 96us/step - loss: 0.1104 - accuracy: 0.8525\n",
            "Epoch 225/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1180 - accuracy: 0.8343\n",
            "Epoch 226/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1125 - accuracy: 0.8497\n",
            "Epoch 227/300\n",
            "712/712 [==============================] - 0s 92us/step - loss: 0.1151 - accuracy: 0.8455\n",
            "Epoch 228/300\n",
            "712/712 [==============================] - 0s 110us/step - loss: 0.1087 - accuracy: 0.8581\n",
            "Epoch 229/300\n",
            "712/712 [==============================] - 0s 112us/step - loss: 0.1128 - accuracy: 0.8525\n",
            "Epoch 230/300\n",
            "712/712 [==============================] - 0s 132us/step - loss: 0.1164 - accuracy: 0.8539\n",
            "Epoch 231/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1113 - accuracy: 0.8610\n",
            "Epoch 232/300\n",
            "712/712 [==============================] - 0s 127us/step - loss: 0.1124 - accuracy: 0.8539\n",
            "Epoch 233/300\n",
            "712/712 [==============================] - 0s 131us/step - loss: 0.1093 - accuracy: 0.8553\n",
            "Epoch 234/300\n",
            "712/712 [==============================] - 0s 110us/step - loss: 0.1107 - accuracy: 0.8567\n",
            "Epoch 235/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1101 - accuracy: 0.8525\n",
            "Epoch 236/300\n",
            "712/712 [==============================] - 0s 125us/step - loss: 0.1132 - accuracy: 0.8525\n",
            "Epoch 237/300\n",
            "712/712 [==============================] - 0s 122us/step - loss: 0.1119 - accuracy: 0.8581\n",
            "Epoch 238/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1114 - accuracy: 0.8553\n",
            "Epoch 239/300\n",
            "712/712 [==============================] - 0s 104us/step - loss: 0.1112 - accuracy: 0.8581\n",
            "Epoch 240/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1087 - accuracy: 0.8581\n",
            "Epoch 241/300\n",
            "712/712 [==============================] - 0s 112us/step - loss: 0.1085 - accuracy: 0.8581\n",
            "Epoch 242/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1158 - accuracy: 0.8483\n",
            "Epoch 243/300\n",
            "712/712 [==============================] - 0s 114us/step - loss: 0.1067 - accuracy: 0.8610\n",
            "Epoch 244/300\n",
            "712/712 [==============================] - 0s 126us/step - loss: 0.1086 - accuracy: 0.8539\n",
            "Epoch 245/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1117 - accuracy: 0.8511\n",
            "Epoch 246/300\n",
            "712/712 [==============================] - 0s 116us/step - loss: 0.1136 - accuracy: 0.8553\n",
            "Epoch 247/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1099 - accuracy: 0.8553\n",
            "Epoch 248/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1131 - accuracy: 0.8455\n",
            "Epoch 249/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1131 - accuracy: 0.8525\n",
            "Epoch 250/300\n",
            "712/712 [==============================] - 0s 109us/step - loss: 0.1091 - accuracy: 0.8610\n",
            "Epoch 251/300\n",
            "712/712 [==============================] - 0s 118us/step - loss: 0.1072 - accuracy: 0.8624\n",
            "Epoch 252/300\n",
            "712/712 [==============================] - 0s 107us/step - loss: 0.1127 - accuracy: 0.8511\n",
            "Epoch 253/300\n",
            "712/712 [==============================] - 0s 131us/step - loss: 0.1109 - accuracy: 0.8497\n",
            "Epoch 254/300\n",
            "712/712 [==============================] - 0s 112us/step - loss: 0.1114 - accuracy: 0.8553\n",
            "Epoch 255/300\n",
            "712/712 [==============================] - 0s 115us/step - loss: 0.1127 - accuracy: 0.8483\n",
            "Epoch 256/300\n",
            "712/712 [==============================] - 0s 128us/step - loss: 0.1098 - accuracy: 0.8596\n",
            "Epoch 257/300\n",
            "712/712 [==============================] - 0s 109us/step - loss: 0.1070 - accuracy: 0.8652\n",
            "Epoch 258/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1121 - accuracy: 0.8539\n",
            "Epoch 259/300\n",
            "712/712 [==============================] - 0s 103us/step - loss: 0.1118 - accuracy: 0.8469\n",
            "Epoch 260/300\n",
            "712/712 [==============================] - 0s 117us/step - loss: 0.1136 - accuracy: 0.8539\n",
            "Epoch 261/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1174 - accuracy: 0.8483\n",
            "Epoch 262/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1070 - accuracy: 0.8553\n",
            "Epoch 263/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1085 - accuracy: 0.8610\n",
            "Epoch 264/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1131 - accuracy: 0.8539\n",
            "Epoch 265/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1072 - accuracy: 0.8581\n",
            "Epoch 266/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1143 - accuracy: 0.8497\n",
            "Epoch 267/300\n",
            "712/712 [==============================] - 0s 118us/step - loss: 0.1106 - accuracy: 0.8567\n",
            "Epoch 268/300\n",
            "712/712 [==============================] - 0s 118us/step - loss: 0.1071 - accuracy: 0.8567\n",
            "Epoch 269/300\n",
            "712/712 [==============================] - 0s 135us/step - loss: 0.1115 - accuracy: 0.8525\n",
            "Epoch 270/300\n",
            "712/712 [==============================] - 0s 114us/step - loss: 0.1137 - accuracy: 0.8581\n",
            "Epoch 271/300\n",
            "712/712 [==============================] - 0s 105us/step - loss: 0.1122 - accuracy: 0.8567\n",
            "Epoch 272/300\n",
            "712/712 [==============================] - 0s 106us/step - loss: 0.1112 - accuracy: 0.8567\n",
            "Epoch 273/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1112 - accuracy: 0.8553\n",
            "Epoch 274/300\n",
            "712/712 [==============================] - 0s 120us/step - loss: 0.1107 - accuracy: 0.8553\n",
            "Epoch 275/300\n",
            "712/712 [==============================] - 0s 117us/step - loss: 0.1109 - accuracy: 0.8567\n",
            "Epoch 276/300\n",
            "712/712 [==============================] - 0s 119us/step - loss: 0.1102 - accuracy: 0.8539\n",
            "Epoch 277/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1122 - accuracy: 0.8441\n",
            "Epoch 278/300\n",
            "712/712 [==============================] - 0s 120us/step - loss: 0.1119 - accuracy: 0.8511\n",
            "Epoch 279/300\n",
            "712/712 [==============================] - 0s 115us/step - loss: 0.1114 - accuracy: 0.8539\n",
            "Epoch 280/300\n",
            "712/712 [==============================] - 0s 108us/step - loss: 0.1114 - accuracy: 0.8567\n",
            "Epoch 281/300\n",
            "712/712 [==============================] - 0s 145us/step - loss: 0.1103 - accuracy: 0.8553\n",
            "Epoch 282/300\n",
            "712/712 [==============================] - 0s 100us/step - loss: 0.1109 - accuracy: 0.8511\n",
            "Epoch 283/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1076 - accuracy: 0.8596\n",
            "Epoch 284/300\n",
            "712/712 [==============================] - 0s 124us/step - loss: 0.1084 - accuracy: 0.8497\n",
            "Epoch 285/300\n",
            "712/712 [==============================] - 0s 136us/step - loss: 0.1134 - accuracy: 0.8553\n",
            "Epoch 286/300\n",
            "712/712 [==============================] - 0s 127us/step - loss: 0.1102 - accuracy: 0.8567\n",
            "Epoch 287/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1086 - accuracy: 0.8553\n",
            "Epoch 288/300\n",
            "712/712 [==============================] - 0s 122us/step - loss: 0.1063 - accuracy: 0.8624\n",
            "Epoch 289/300\n",
            "712/712 [==============================] - 0s 114us/step - loss: 0.1085 - accuracy: 0.8567\n",
            "Epoch 290/300\n",
            "712/712 [==============================] - 0s 125us/step - loss: 0.1111 - accuracy: 0.8581\n",
            "Epoch 291/300\n",
            "712/712 [==============================] - 0s 118us/step - loss: 0.1099 - accuracy: 0.8567\n",
            "Epoch 292/300\n",
            "712/712 [==============================] - 0s 144us/step - loss: 0.1094 - accuracy: 0.8596\n",
            "Epoch 293/300\n",
            "712/712 [==============================] - 0s 136us/step - loss: 0.1126 - accuracy: 0.8511\n",
            "Epoch 294/300\n",
            "712/712 [==============================] - 0s 109us/step - loss: 0.1106 - accuracy: 0.8610\n",
            "Epoch 295/300\n",
            "712/712 [==============================] - 0s 114us/step - loss: 0.1172 - accuracy: 0.8371\n",
            "Epoch 296/300\n",
            "712/712 [==============================] - 0s 114us/step - loss: 0.1084 - accuracy: 0.8610\n",
            "Epoch 297/300\n",
            "712/712 [==============================] - 0s 102us/step - loss: 0.1075 - accuracy: 0.8680\n",
            "Epoch 298/300\n",
            "712/712 [==============================] - 0s 111us/step - loss: 0.1077 - accuracy: 0.8553\n",
            "Epoch 299/300\n",
            "712/712 [==============================] - 0s 121us/step - loss: 0.1080 - accuracy: 0.8624\n",
            "Epoch 300/300\n",
            "712/712 [==============================] - 0s 113us/step - loss: 0.1099 - accuracy: 0.8596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SGw3b6UkiGj",
        "colab_type": "code",
        "outputId": "5aa718cb-afdc-4b5b-83c1-602976e6beb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_y_pred = model.predict(test_X)\n",
        "test_y_pred = np.round(test_y_pred)\n",
        "mae = mean_absolute_error(test_y,test_y_pred)\n",
        "\n",
        "print(\"Mean absolute error is : {}\".format(mae))\n",
        "a = accuracy_score(test_y,test_y_pred)\n",
        "print(\"Accuracy score is : {} \".format(a))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean absolute error is : 0.2011173184357542\n",
            "Accuracy score is : 0.7988826815642458 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "850d8a9c160d2255c1ba1848c68a1a05d53bf43b",
        "id": "BeqXDk9cabQP",
        "colab_type": "code",
        "outputId": "9e7492cb-8871-4ec4-b108-c70b3b50105f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Using XGboost \n",
        "model1 = XGBRegressor(n_estimates = 1000,learning_rate=0.1)\n",
        "model1.fit(train_X,train_y)\n",
        "y_pred1 = model1.predict(test_X)\n",
        "y_pred1 = np.round(y_pred1)\n",
        "\n",
        "print(\"With XGBoost\")\n",
        "mae = mean_absolute_error(test_y,y_pred1)\n",
        "print(\"Mean absolute error is : {}\".format(mae))\n",
        "a = accuracy_score(test_y,y_pred1)\n",
        "print(\"Accuracy score is : {} \".format(a))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14:20:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "With XGBoost\n",
            "Mean absolute error is : 0.2122905027932961\n",
            "Accuracy score is : 0.7877094972067039 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "391c401bfc6847ce760d41b773704a8d412c3afa",
        "id": "jPYZevSqabQS",
        "colab_type": "code",
        "outputId": "1675c705-26f3-4e61-8fb8-31dd62b6f255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Using RandomForestRegressor\n",
        "model2 = RandomForestRegressor(random_state=1)\n",
        "model2.fit(train_X,train_y)\n",
        "y_pred2 = model2.predict(test_X)\n",
        "y_pred2 = np.round(y_pred2)\n",
        "\n",
        "print(\"With RandomForestRegressor\")\n",
        "mae = mean_absolute_error(test_y,y_pred2)\n",
        "print(\"Mean absolute error is : {}\".format(mae))\n",
        "a = accuracy_score(test_y,y_pred2)\n",
        "print(\"Accuracy score is : {} \".format(a))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With RandomForestRegressor\n",
            "Mean absolute error is : 0.2122905027932961\n",
            "Accuracy score is : 0.7877094972067039 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "775f80d7501370e57e18d3815b4eabe1c5678d3e",
        "id": "ANaWyyyiabQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using DecisionTreeRegressor \n",
        "model3 = DecisionTreeRegressor(random_state=1,)\n",
        "model3.fit(train_X,train_y)\n",
        "y_pred3 = model3.predict(test_X)\n",
        "y_pred3 = np.round(y_pred3)\n",
        "\n",
        "print(\"With DecisionTreeeRegressor\")\n",
        "mae = mean_absolute_error(test_y,y_pred3)\n",
        "print(\"Mean absolute error is : {}\".format(mae))\n",
        "a = accuracy_score(test_y,y_pred3)\n",
        "print(\"Accuracy score is : {} \".format(a))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "f08fefa4839c25c4e432101cb7dfcb4da2fef94b",
        "id": "JS6HI2YmabQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_data)\n",
        "predictions = np.round(predictions)\n",
        "predictions = predictions.astype(int)\n",
        "predictions = predictions.ravel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "84196b7d56e08fca0f4cf713a4d37e5f64d765f2",
        "id": "HjDuIIXDabQe",
        "colab_type": "text"
      },
      "source": [
        "### sending predictions to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c1d3deace46f0691de38a3cd700d9f3290667922",
        "id": "3AS7YC6kabQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.DataFrame({\n",
        "    \"PassengerId\" : orig_test['PassengerId'],\n",
        "    \"Survived\" : predictions\n",
        "})\n",
        "submission.to_csv('drive/My Drive/Colab Notebooks/titanic_pred.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}